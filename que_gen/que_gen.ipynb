{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question generation serving on Sagemaker\n",
    "Code is from https://colab.research.google.com/drive/1_2_mS5l29QHI1pXaqa4YLzAO5xm-HmH9?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nCollecting gdown\n  Downloading https://files.pythonhosted.org/packages/fc/7a/5a892d25b0a105b9d825cdd0c69d2742b83e00f247053f50a7e0a5405439/gdown-3.11.1.tar.gz\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from gdown) (3.0.12)\nRequirement already satisfied: tqdm in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from gdown) (4.46.1)\nRequirement already satisfied: requests[socks] in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from gdown) (2.23.0)\nRequirement already satisfied: six in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from requests[socks]->gdown) (2020.4.5.2)\nRequirement already satisfied: idna<3,>=2.5 in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from requests[socks]->gdown) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from requests[socks]->gdown) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /Users/hunkim/work/sagemaker-aihub/.venv/lib/python3.7/site-packages (from requests[socks]->gdown) (3.0.4)\nCollecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\" (from requests[socks]->gdown)\n  Downloading https://files.pythonhosted.org/packages/8d/59/b4572118e098ac8e46e399a1dd0f2d85403ce8bbaad9ec79373ed6badaf9/PySocks-1.7.1-py3-none-any.whl\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /Users/hunkim/Library/Caches/pip/wheels/32/8e/8d/0fec7d933759a0880dc34be487e1d92c51ba8fdf4e9f265ff7\nSuccessfully built gdown\nInstalling collected packages: gdown, PySocks\nSuccessfully installed PySocks-1.7.1 gdown-3.11.1\n\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nsagemaker           1.60.2    \ntokenizers          0.7.0     \ntransformers        2.11.0    \n\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "!pip install -q sagemaker sagemaker[local]\n",
    "!pip install gdown\n",
    "\n",
    "# Install `transformers` from master\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip list | grep -E 'transformers|tokenizers|sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:root:pandas failed to import. Analytics features will be impaired or broken.\n"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/hunkim-que-gen'\n",
    "role = \"arn:aws:iam::294038372338:role/hunkimSagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading...\nFrom: https://drive.google.com/uc?id=1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\nTo: /Users/hunkim/work/sagemaker-aihub/que_gen/t5_que_gen.zip\n1.65GB [01:09, 23.8MB/s]\nArchive:  t5_que_gen.zip\nreplace t5_que_gen_model/t5_base_tok_que_gen/spiece.model? [y]es, [n]o, [A]ll, [N]one, [r]ename:^C\ntar: t5_que_gen_modelt5_que_gen_model: Cannot stat: No such file or directory\ntar: Error exit delayed from previous errors.\n"
    }
   ],
   "source": [
    "!gdown -O  t5_que_gen.zip --id 1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\n",
    "!unzip t5_que_gen.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "a t5_ans_gen_model\na t5_ans_gen_model/t5_base_ans_gen\na t5_ans_gen_model/t5_base_tok_ans_gen\na t5_ans_gen_model/t5_base_tok_ans_gen/added_tokens.json\na t5_ans_gen_model/t5_base_tok_ans_gen/tokenizer_config.json\na t5_ans_gen_model/t5_base_tok_ans_gen/special_tokens_map.json\na t5_ans_gen_model/t5_base_tok_ans_gen/spiece.model\na t5_ans_gen_model/t5_base_ans_gen/config.json\na t5_ans_gen_model/t5_base_ans_gen/pytorch_model.bin\na t5_que_gen_model\na t5_que_gen_model/t5_base_tok_que_gen\na t5_que_gen_model/logs.zip\na t5_que_gen_model/t5_base_que_gen\na t5_que_gen_model/t5_base_que_gen/config.json\na t5_que_gen_model/t5_base_que_gen/pytorch_model.bin\na t5_que_gen_model/t5_base_tok_que_gen/added_tokens.json\na t5_que_gen_model/t5_base_tok_que_gen/tokenizer_config.json\na t5_que_gen_model/t5_base_tok_que_gen/special_tokens_map.json\na t5_que_gen_model/t5_base_tok_que_gen/spiece.model\n"
    }
   ],
   "source": [
    "!mkdir -p model\n",
    "!tar cvfz model/model.tar.gz t5_ans_gen_model t5_que_gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\ninput spec (in this case, just an S3 path): s3://sagemaker-us-west-2-294038372338/sagemaker/hunkim-que-gen/model.tar.gz\n"
    }
   ],
   "source": [
    "model = sagemaker_session.upload_data(path='model', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(model+\"/model.tar.gz\"))\n",
    "model_data = model+\"/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m chain\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mstring\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m punctuation\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnltk\u001b[39;49;00m\nnltk.download(\u001b[33m'\u001b[39;49;00m\u001b[33mpunkt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnltk\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtokenize\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m sent_tokenize\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset, DataLoader\n\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_linear_schedule_with_warmup\n)\n\n\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mQueGenerator\u001b[39;49;00m():\n  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model_dir=\u001b[33m\"\u001b[39;49;00m\u001b[33m./\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n    \u001b[36mself\u001b[39;49;00m.que_model = T5ForConditionalGeneration.from_pretrained(model_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/t5_que_gen_model/t5_base_que_gen/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[36mself\u001b[39;49;00m.ans_model = T5ForConditionalGeneration.from_pretrained(model_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/t5_ans_gen_model/t5_base_ans_gen/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\n    \u001b[36mself\u001b[39;49;00m.que_tokenizer = T5Tokenizer.from_pretrained(model_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/t5_que_gen_model/t5_base_tok_que_gen/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[36mself\u001b[39;49;00m.ans_tokenizer = T5Tokenizer.from_pretrained(model_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/t5_ans_gen_model/t5_base_tok_ans_gen/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \n    \u001b[36mself\u001b[39;49;00m.device = \u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n    \n    \u001b[36mself\u001b[39;49;00m.que_model = \u001b[36mself\u001b[39;49;00m.que_model.to(\u001b[36mself\u001b[39;49;00m.device)\n    \u001b[36mself\u001b[39;49;00m.ans_model = \u001b[36mself\u001b[39;49;00m.ans_model.to(\u001b[36mself\u001b[39;49;00m.device)\n  \n  \u001b[34mdef\u001b[39;49;00m \u001b[32mgenerate\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text):\n    answers = \u001b[36mself\u001b[39;49;00m._get_answers(text)\n    questions = \u001b[36mself\u001b[39;49;00m._get_questions(text, answers)\n    output = [{\u001b[33m'\u001b[39;49;00m\u001b[33manswer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: ans, \u001b[33m'\u001b[39;49;00m\u001b[33mquestion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: que} \u001b[34mfor\u001b[39;49;00m ans, que \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(answers, questions)]\n    \u001b[34mreturn\u001b[39;49;00m output\n  \n  \u001b[34mdef\u001b[39;49;00m \u001b[32m_get_answers\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text):\n    \u001b[37m# split into sentences\u001b[39;49;00m\n    sents = sent_tokenize(text)\n\n    examples = []\n    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(sents)):\n      input_ = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n      \u001b[34mfor\u001b[39;49;00m j, sent \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(sents):\n        \u001b[34mif\u001b[39;49;00m i == j:\n            sent = \u001b[33m\"\u001b[39;49;00m\u001b[33m[HL] \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m [HL]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % sent\n        input_ = \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (input_, sent)\n        input_ = input_.strip()\n      input_ = input_ + \u001b[33m\"\u001b[39;49;00m\u001b[33m </s>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n      examples.append(input_)\n    \n    batch = \u001b[36mself\u001b[39;49;00m.ans_tokenizer.batch_encode_plus(examples, max_length=\u001b[34m512\u001b[39;49;00m, pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n      outs = \u001b[36mself\u001b[39;49;00m.ans_model.generate(input_ids=batch[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device), \n                                attention_mask=batch[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device), \n                                max_length=\u001b[34m32\u001b[39;49;00m,\n                                \u001b[37m# do_sample=False,\u001b[39;49;00m\n                                \u001b[37m# num_beams = 4,\u001b[39;49;00m\n                                )\n    dec = [\u001b[36mself\u001b[39;49;00m.ans_tokenizer.decode(ids, skip_special_tokens=\u001b[34mFalse\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m ids \u001b[35min\u001b[39;49;00m outs]\n    answers = [item.split(\u001b[33m'\u001b[39;49;00m\u001b[33m[SEP]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m dec]\n    answers = chain(*answers)\n    answers = [ans.strip() \u001b[34mfor\u001b[39;49;00m ans \u001b[35min\u001b[39;49;00m answers \u001b[34mif\u001b[39;49;00m ans != \u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    \u001b[34mreturn\u001b[39;49;00m answers\n  \n  \u001b[34mdef\u001b[39;49;00m \u001b[32m_get_questions\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, answers):\n    examples = []\n    \u001b[34mfor\u001b[39;49;00m ans \u001b[35min\u001b[39;49;00m answers:\n      input_text = \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m [SEP] \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m </s>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (ans, text)\n      examples.append(input_text)\n    \n    batch = \u001b[36mself\u001b[39;49;00m.que_tokenizer.batch_encode_plus(examples, max_length=\u001b[34m512\u001b[39;49;00m, pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n      outs = \u001b[36mself\u001b[39;49;00m.que_model.generate(input_ids=batch[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device), \n                                attention_mask=batch[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device), \n                                max_length=\u001b[34m32\u001b[39;49;00m,\n                                num_beams = \u001b[34m4\u001b[39;49;00m)\n    dec = [\u001b[36mself\u001b[39;49;00m.que_tokenizer.decode(ids, skip_special_tokens=\u001b[34mFalse\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m ids \u001b[35min\u001b[39;49;00m outs]\n    \u001b[34mreturn\u001b[39;49;00m dec\n\nJSON_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n\nlogger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading the model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\n    que_generator = QueGenerator(model_dir)\n\n    \u001b[34mreturn\u001b[39;49;00m que_generator\n\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\n        input_data = json.loads(serialized_input_data)\n        \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m input_data:\n            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m has to be set.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n        \u001b[34mreturn\u001b[39;49;00m input_data\n    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept=JSON_CONTENT_TYPE):\n    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[34mif\u001b[39;49;00m accept == JSON_CONTENT_TYPE:\n        \u001b[34mreturn\u001b[39;49;00m json.dumps(prediction_output), accept\n    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + accept)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mGenerating text based on input parameters.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[34mreturn\u001b[39;49;00m model.generate(input_data[\u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n\u001b[37m# For testing, make sure you run the infer.py and store the model outputs to S3\u001b[39;49;00m\n\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n    model_dir=\u001b[33m\"\u001b[39;49;00m\u001b[33m./.model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n\n    \u001b[37m# Please run train and zip and store the model directory to S3\u001b[39;49;00m\n    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n    s3 = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    s3.download_file(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-us-west-2-294038372338\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n    \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker/hunkim-que-gen/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\n    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n    tar = tarfile.open(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n    tar.extractall(model_dir)\n    tar.close()\n\n    \u001b[36minput\u001b[39;49;00m = {\u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mGood to see you. My name is Sung Kim.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\n    model = model_fn(model_dir)\n    \u001b[36mprint\u001b[39;49;00m(predict_fn(\u001b[36minput\u001b[39;49;00m, model))\n"
    }
   ],
   "source": [
    "!pygmentize code/que_gen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                     role=role,\n",
    "                     framework_version='1.5.0',\n",
    "                     entry_point='que_gen.py',\n",
    "                     source_dir='code',\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\nWARNING:sagemaker:Using already existing model: pytorch-inference-2020-06-12-03-31-58-602\n-------------------!pytorch-inference-2020-06-12-03-31-58-602\n"
    }
   ],
   "source": [
    "#predictor = model.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge')\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='local')\n",
    "\n",
    "# Get the end point\n",
    "endpoint = predictor.endpoint\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'answer': 'Hong Kong University of Science and Technology', 'question': 'Where is Sunghun Kim an Associate Professor of Computer Science?'}, {'answer': 'Software Engineering and Machine Learning', 'question': \"What is Sunghun Kim's core research area?\"}, {'answer': 'TSE, ICSE, FSE, AAAI, SOSP and ISSTA', 'question': 'Where does Sunghun Kim publish his work?'}, {'answer': 'ACM SIGSOFT Distinguished Paper Award', 'question': 'What award has Sunghun Kim won four times?'}, {'answer': 'Microsoft Software Innovation Awards', 'question': 'What awards did Kim receive in 2010 and 2011?'}, {'answer': 'http://www.cse.ust.hk/ ⁇ hunkim', 'question': 'Where can you find more information about Sunghun Kim?'}]\n"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Sunghun Kim is an Associate Professor of Computer Science at the Hong Kong University of Science and Technology. \n",
    "\n",
    "His core research area is Software Engineering and Machine Learning \n",
    "focusing on software evolution, program analysis and empirical studies. \n",
    "He publishes his work on top venues such as TSE, ICSE, FSE, AAAI, SOSP and ISSTA. \n",
    "He is a four-time winner of the ACM SIGSOFT Distinguished Paper Award \n",
    "(ICSE 2007, ASE 2012, ICSE 2013 and ISSTA 2014). \n",
    "Besides, he received various awards including 2010 and \n",
    "2011 Microsoft Software Innovation Awards and 2011 Google Faculty Research Award. \n",
    "Further information is available at http://www.cse.ust.hk/~hunkim.\n",
    "\"\"\"\n",
    "input = {\n",
    "    'text': text\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Worker died.\"\n}\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2020-06-12-03-31-58-602 in account 294038372338 for more information.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-6c1e691cc437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mguen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/sagemaker-aihub/.venv/lib/python3.7/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/sagemaker-aihub/.venv/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/sagemaker-aihub/.venv/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Worker died.\"\n}\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2020-06-12-03-31-58-602 in account 294038372338 for more information."
     ]
    }
   ],
   "source": [
    "guen=\"\"\"Guen Do-Gyun is a representative of the primer.\n",
    "\n",
    "Guen Do-Gyun graduated from Kyungpook National University's Department of Electronic Engineering in 1986 and joined Kia Motors and Dacom for 11 years as a computer programmer. In 1997, he left Dacom's General Research Institute, where he worked for 10 years at the age of 35, collected severance and charter payments, and sold the keys received from Dacom to make about 100 million won. In 1997, the security company Initech was established, and in 1998, the electronic payment company Inisis was established. Kwon Do-gyun, who first came into contact with the Internet in 1994, began to be interested in e-commerce and e-payment infrastructure, and developed a convenient and highly secure e-payment technology and started services. Starting with Inisisys' INIpay service, it has maintained the No. 1 position in the industry with 70% market share in 2001.\n",
    "\"\"\"\n",
    "input = {\n",
    "    'text': guen\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'[{\"answer\": \"Channy Yun\", \"question\": \"Who is the Principal Technical Evangelist at Amazon Web Services?\"}, {\"answer\": \"17,000\", \"question\": \"How many members does the AWS Korea User Group have?\"}, {\"answer\": \"Cloud native architecture\", \"question\": \"What type of architecture does Channy Yun have a special interest in?\"}]'\n"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "channy=\"\"\"\n",
    "Channy Yun is a Principal Technical Evangelist at Amazon Web Services and works with Korean developers to enable them to use AWS Cloud services. He focuses on engaging technical audiences such as developers, communities, and AWS Korea User Group with over 17,000 members and shared knowledge of the latest AWS cloud services in blog posts, social media, and public speaking. He has a special interest in cloud native architecture such as Serverless and Microservices based on DevOps.\n",
    "\"\"\"\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "input = {\n",
    "    'text': channy\n",
    "}\n",
    "payload = json.dumps(input)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint, \n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\" ,\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "print(response['Body'].read())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitvenvvenvfbc6d215d2384daca7c44542d667cdba",
   "display_name": "Python 3.7.3 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}